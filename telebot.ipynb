{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4kRo42h5B18k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5271e758-42b0-4f92-be0f-7a5425dbaa34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyTelegramBotAPI in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyTelegramBotAPI) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (1.24.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: stop-words in /usr/local/lib/python3.7/dist-packages (2018.7.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyTelegramBotAPI\n",
        "\n",
        "import telebot\n",
        "from telebot import types\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import one_hot\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install spacy\n",
        "!{sys.executable} -m spacy download en\n",
        "\n",
        "import spacy\n",
        "\n",
        "!pip install stop-words\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "from urllib.parse import urlparse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = telebot.TeleBot('5260450850:AAGkp80bHug3CzO0AVj41IEoVNDocOJLfGk');"
      ],
      "metadata": {
        "id": "OHBF5Pk8vgSa"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parser(link):\n",
        "    #link='https://www.foxnews.com/politics/walt-disney-would-be-rolling-over-in-his-grave-over-companys-wokeness-florida-park-goer-says'\n",
        "    req = requests.get(link).text\n",
        "    soup = BeautifulSoup(req, \"lxml\")\n",
        "    head = soup.find('head')\n",
        "    zagolovok = head.find('title').text\n",
        "    textp = soup.find_all('div', attrs={'class': \"article__text\"})\n",
        "    if (len(textp)) == 0:\n",
        "        textp = soup.find_all('span', attrs={'class': 'intro'})\n",
        "    if (len(textp)) == 0:\n",
        "        text = soup.find_all('span', attrs={'class': 'intro'})\n",
        "    if (len(textp)) == 0:\n",
        "        textp = soup.find_all('p')\n",
        "    \n",
        "    # domain = urlparse(link).netloc\n",
        "    # if (int(domain.find(\"www.\")) != -1):\n",
        "    #   domain = domain[4:]\n",
        "    # url = link.split(\"//\")[-1].split(\"/\")[0].split('?')[0]\n",
        "    # show = \"https://input.payapi.io/v1/api/fraud/domain/age/\" + domain\n",
        "    # r = requests.get(show, verify = False)\n",
        "    # soupp = BeautifulSoup(r.text, \"lxml\")\n",
        "    # if requests.get(link).status_code ==200:\n",
        "    #     age = soupp.find('body').text\n",
        "    #     str = int(age.find(\"Reg\"))\n",
        "    #     end = int(age.find(\"}\"))\n",
        "    #     # print(age[str:(end - 1)])\n",
        "    #     age=age[str:(end - 1)]\n",
        "    #     data = r.text\n",
        "    #     jsonToPython = json.loads(data)\n",
        "    # else:\n",
        "    #   age=np.nan\n",
        "\n",
        "    for j in range(len(textp)):\n",
        "      textp[j]=textp[j].text\n",
        "\n",
        "    textp=''.join(textp)\n",
        "    zagolovok=''.join(zagolovok)\n",
        "\n",
        "    return textp, zagolovok\n",
        "    "
      ],
      "metadata": {
        "id": "76sM8ReC0UXM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "paqwcWcF0eEg",
        "outputId": "3941dfde-3a97-49a2-81da-60c9d2b91b1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-674d2322-942a-408d-82ea-957e1bca2cdf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-674d2322-942a-408d-82ea-957e1bca2cdf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.h5 to Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.h5\n",
            "Saving Ð¼Ð¾Ð´ÐµÐ»ÑŒ_Ð´Ð»Ñ_Ð°Ð½Ð°Ð»Ð¸Ð·Ð°_Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°.h5 to Ð¼Ð¾Ð´ÐµÐ»ÑŒ_Ð´Ð»Ñ_Ð°Ð½Ð°Ð»Ð¸Ð·Ð°_Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ð°Ð½Ð°Ð»Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°\n",
        "def headlineAnalis(headline):\n",
        "    model_for_headline = keras.models.load_model('Ð¼Ð¾Ð´ÐµÐ»ÑŒ_Ð´Ð»Ñ_Ð°Ð½Ð°Ð»Ð¸Ð·Ð°_Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°.h5')\n",
        "\n",
        "    df_h = pd.DataFrame({'Headline' : [headline]})\n",
        "\n",
        "    oh_headlines = [one_hot(words, 5000) for words in df_h['Headline']]\n",
        "    x = pad_sequences(oh_headlines, padding='pre', maxlen=30)\n",
        "\n",
        "    headLines = model_for_headline.predict(x)\n",
        "    df_h.Headline = headLines\n",
        "\n",
        "    return df_h"
      ],
      "metadata": {
        "id": "81Fs1NlM0xWY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸ Ð¸ Ð¿Ñ€Ð¸Ð²ÐµÐµÐ½Ð¸Ðµ Ðº Ð½Ð¸Ð¶Ð½ÐµÐ¼Ñƒ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ñƒ\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = {33: ' ', 34: ' ', 35: ' ', 36: ' ', 37: ' ', 38: ' ', 39: ' ', 40: ' ', 41: ' ', 42: ' ',\n",
        "             43: ' ', 44: ' ', 45: ' ', 46: ' ', 47: ' ', 58: ' ', 59: ' ', 60: ' ', 61: ' ', 62: ' ',\n",
        "             63: ' ', 64: ' ', 91: ' ', 92: ' ', 93: ' ', 94: ' ', 95: ' ', 96: ' ', 123: ' ', 124: ' ', 125: ' ', 126: ' '}\n",
        "    return text.translate(table)\n",
        "\n",
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = remove_punct(text)\n",
        "\n",
        "    signs = ['?', ',', ':', '!', '.', ';', 'Â«', 'Â»', '(', ')', '*', '+', '-', '^', '|', '=', \"â€™\", \"â€˜\",  'â€', 'â€”', '@', '>', '<', 'â€“', '&', '%', 'â€¢', '/', 'â€œ']\n",
        "    for punct_sign in signs:\n",
        "        text = text.replace(punct_sign, ' ')\n",
        "\n",
        "    text = text.replace('\\n', '')\n",
        "    text = text.replace('\\r', '')\n",
        "    text = re.sub(r\"\\d+\", \"\", text, flags=re.UNICODE)\n",
        "    text = text.replace(\"  \" , \" \")\n",
        "    text = text.replace(\"  \" , \" \")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "oNFjFfnZ00KN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ð½Ð°Ñ€ÐµÑ‡Ð¸Ð¹\n",
        "\n",
        "def amountAdverbs(text):\n",
        "    amountADV = 0\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "    for i in tags:\n",
        "        if len(i) == 2:\n",
        "            if i[1] == 'RB':\n",
        "                amountADV += 1\n",
        "        else:\n",
        "            continue\n",
        "    return amountADV"
      ],
      "metadata": {
        "id": "nZT660zH03g2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ð»ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²\n",
        "\n",
        "def removeStopWords_and_lemmitizeText(text):\n",
        "    lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n",
        "    doc = lemmatizer(text)\n",
        "    new_text = \" \".join([token.lemma_ for token in doc\\\n",
        "                         if token.lemma_ not in get_stop_words('english')\\\n",
        "                         and (token.lemma_ != \"-PRON-\")\\\n",
        "                         and (len(token.lemma_) != 1 or token.lemma_ == 'i')])\n",
        "    # new_text = new_text.replace(\"-PRON-\", '')\n",
        "    # new_text = new_text.replace(\"s\", '')\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "Oc6DTpxM06d-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¸ ÑÑƒÐ±ÑŠÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ\n",
        "\n",
        "def polarityAndSubjectivityOfText(text):\n",
        "    pol = TextBlob(text).polarity\n",
        "    sub = TextBlob(text).subjectivity\n",
        "    return pol, sub"
      ],
      "metadata": {
        "id": "2eQS4lQo09OX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ\n",
        "\n",
        "def toVecText(text):\n",
        "\n",
        "    d = {'text': [text]}\n",
        "    df1 = pd.DataFrame(d)\n",
        "\n",
        "    oh_text = [one_hot(words, 5000) for words in df1.text]\n",
        "    embedded_doc = pad_sequences(oh_text, padding='pre', maxlen=800)\n",
        "    df_text = pd.DataFrame(embedded_doc)\n",
        "\n",
        "    return df_text\n",
        "\n",
        "# oh_text = one_hot(text, 5000)\n",
        "# embedded_doc = pad_sequences(oh_text, padding='pre', maxlen=800)\n",
        "# df_text = pd.DataFrame(embedded_doc)\n",
        "# df_new = pd.concat([df.drop(['Body_02'], axis = 1), df_text], axis = 1)"
      ],
      "metadata": {
        "id": "zBprUuQq1AFV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from numpy.ma.core import logical_not\n",
        "def allTextPreparation(text):\n",
        "\n",
        "    # Ñ‡Ð¸ÑÑ‚Ð¸Ð¼ Ð¾Ñ‚ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸ + Ð½Ð¸Ð¶Ð³Ð¸Ð¹ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€\n",
        "    text = cleanText(text)\n",
        "\n",
        "    # ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð½Ð°Ñ€ÐµÑ‡Ð¸Ð¹\n",
        "    amountAdv = amountAdverbs(text)\n",
        "\n",
        "    # Ð»ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²\n",
        "    text = removeStopWords_and_lemmitizeText(text)\n",
        "\n",
        "    # Ð¿Ð¾Ð»ÑÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¸ ÑÑƒÐ±ÑŠÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ\n",
        "    l_pol_sub = polarityAndSubjectivityOfText(text)\n",
        "\n",
        "    polarity = l_pol_sub[0]\n",
        "    subjectivity = l_pol_sub[1]\n",
        "\n",
        "    # Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°\n",
        "    vec_text = toVecText(text)\n",
        "\n",
        "    data_ = {'Amount Adverbs' : [amountAdv], \n",
        "             'Polarity' : [polarity],\n",
        "             'Subjectivity' : [subjectivity]}\n",
        "\n",
        "    df_features = pd.DataFrame(data_)\n",
        "\n",
        "    df_new = pd.concat([df_features, vec_text], axis = 1)\n",
        "\n",
        "    return df_new\n"
      ],
      "metadata": {
        "id": "h5ey7yf31CFb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataPreparation(data):\n",
        "    # data = [text, headline]\n",
        "    text = data[0]\n",
        "    headline = data[1]\n",
        "    df_text = allTextPreparation(text)\n",
        "    df_headline = headlineAnalis(headline)\n",
        "\n",
        "    df = pd.concat([df_headline, df_text], axis = 1)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "6AY5yNPa1E7O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataPrediction(data_df):\n",
        "    finalModel = keras.models.load_model('Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.h5')\n",
        "    result = finalModel.predict(data_df)\n",
        "    result = int(np.around(result))\n",
        "\n",
        "    if result == 0:\n",
        "        return \"Ð¡ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð´Ð¾Ð»ÐµÐ¹ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ - Ñ„ÐµÐ¹Ðº.\"\n",
        "    elif result == 1:\n",
        "        return \"Ð¡ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð´Ð¾Ð»ÐµÐ¹ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ñ„ÐµÐ¹ÐºÐ¾Ð¼.\"\n",
        "    "
      ],
      "metadata": {
        "id": "ECw2dlAv1Hev"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fakeNewsChecking(url):\n",
        "    # Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ url Ð² Ð¿Ð°Ñ€ÑÐµÑ€, Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ\n",
        "    data = parser(url)\n",
        "    df = dataPreparation(data)\n",
        "    res = dataPrediction(df)\n",
        "    return res"
      ],
      "metadata": {
        "id": "x5z13vEZ1Lf4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "c9CeBpsyC8_L"
      },
      "outputs": [],
      "source": [
        "\n",
        "@bot.message_handler(commands=[\"start\", \"restart\"])\n",
        "def start(m, res=False):\n",
        "        bot.send_message(m.chat.id, 'ÐŸÑ€Ð¸Ð²ÐµÑ‚, Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ!\\n\\nÐœÐµÐ½Ñ Ð·Ð¾Ð²ÑƒÑ‚ FakeBot. Ð¯ Ð·Ð°Ñ‰Ð¸Ñ‰Ñƒ Ñ‚ÐµÐ±Ñ Ð¾Ñ‚ Ñ„ÐµÐ¹ÐºÐ¾Ð² Ð² ÑÐµÑ‚Ð¸. Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð¼Ð½Ðµ Ð»Ð¸ÑˆÑŒ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ ÑÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¿Ð¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ, Ð½ÑƒÐ¶Ð´Ð°ÑŽÑ‰ÑƒÑŽÑÑ Ð² Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐµ.\\n\\nÐ¡Ð¿Ð¸ÑÐ¾Ðº ÐºÐ¾Ð¼Ð°Ð½Ð´:\\\n",
        "        \\n/restart â€“ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð±Ð¾Ñ‚a\\\n",
        "        \\n/help â€“ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð³Ð¸Ð´ Ð¿Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°Ð¼\\\n",
        "        \\n/check â€“ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ\\\n",
        "        \\n/more â€“ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¾Ð±Ð¾ Ð¼Ð½Ðµ Ð¸ Ð¼Ð¾Ð¸Ñ… ÑÐ¾Ð·Ð´Ð°Ñ‚ÐµÐ»ÑÑ…')\n",
        "@bot.message_handler(commands=[\"help\"])\n",
        "def help(message):\n",
        "  bot.send_message(message.from_user.id, 'Ð¡Ð¿Ð¸ÑÐ¾Ðº ÐºÐ¾Ð¼Ð°Ð½Ð´:\\\n",
        "  \\n/restart â€“ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð±Ð¾Ñ‚a\\\n",
        "  \\n/help â€“ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð³Ð¸Ð´ Ð¿Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°Ð¼\\\n",
        "  \\n/check â€“ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ\\\n",
        "  \\n/more â€“ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¾Ð±Ð¾ Ð¼Ð½Ðµ Ð¸ Ð¼Ð¾Ð¸Ñ… ÑÐ¾Ð·Ð´Ð°Ñ‚ÐµÐ»ÑÑ…')\n",
        "# @bot.message_handler(commands=[\"start\"])\n",
        "# def start(m, res=False):\n",
        "#         markup=types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
        "#         item1=types.KeyboardButton(\"ÐÐ°Ñ‡Ð°Ñ‚ÑŒ\")\n",
        "#         item2=types.KeyboardButton(\"Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ\")\n",
        "#         item3=types.KeyboardButton(\"ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ\")\n",
        "#         markup.add(item1)\n",
        "#         markup.add(item2)\n",
        "#         markup.add(item3)\n",
        "#         bot.send_message(m.chat.id, 'ÐÐ°Ð¶Ð¼Ð¸: \\nÐÐ°Ñ‡Ð°Ñ‚ÑŒ(Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð¾ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¾Ð¹)\\nÐ˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ(Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑƒÐ·Ð°Ð½Ñ‚ÑŒ Ð¾ Ð½Ð°Ñ)\\nÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ(Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð¾ Ð±Ð¾Ñ‚Ðµ)',  reply_markup=markup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OqBOetzpBl2R"
      },
      "outputs": [],
      "source": [
        "@bot.message_handler(commands=[\"more\"])\n",
        "def help(message):\n",
        "  bot.send_message(message.from_user.id, 'ÐœÐµÐ½Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð° Ð¿Ð°Ñ€Ð° Ñ€ÐµÐ±ÑÑ‚, Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¾Ð²-Ð»ÑŽÐ±Ð¸Ñ‚ÐµÐ»ÐµÐ¹, Ð¸Ð· Ð»Ð¸Ñ†ÐµÑ 1511 Ð¿Ñ€Ð¸ ÐœÐ˜Ð¤Ð˜ Ð² Ð³Ð¾Ñ€Ð¾Ð´Ðµ ÐœÐ¾ÑÐºÐ²Ðµ. ÐžÐ½Ð¸ Ð¾Ñ‡ÐµÐ½ÑŒ ÑÑ‚Ð°Ñ€Ð°Ð»Ð¸ÑÑŒ Ð¸ Ð½Ð°Ð´ÐµÑŽÑ‚ÑÑ, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾Ñ‚ Ð±Ð¾Ñ‚ Ð±ÑƒÐ´ÐµÑ‚ Ñ‚ÐµÐ±Ðµ Ð¿Ð¾Ð»ÐµÐ·ÐµÐ½. ðŸ˜Š')\n",
        "@bot.message_handler(commands=\"check\")\n",
        "def rabota(message):\n",
        "  bot.send_message(message.from_user.id, 'Ð’Ð²ÐµÐ´Ð¸ ÑÑÑ‹Ð»ÐºÑƒ Ð½Ð° Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ')\n",
        "  bot.register_next_step_handler(message, test);\n",
        "def test(message):\n",
        "  link=message.text\n",
        "  if requests.get(link).status_code==200:\n",
        "    link=message.text\n",
        "    bot.send_message(message.chat.id, 'ÐŸÐ¾Ð´Ð¾Ð¶Ð´Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐµÐºÑƒÐ½Ð´, Ð¸Ð´ÑƒÑ‚ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ...')\n",
        "    predict=fakeNewsChecking(link)\n",
        "    bot.send_message(message.chat.id,text=predict)\n",
        "  else:\n",
        "    bot.send_message(message.chat.id,'C ÑÑ‚Ð¾Ð¹ ÑÑÑ‹Ð»ÐºÐ¾Ð¹ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð½Ðµ Ñ‚Ð°Ðº. Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, ÑÐ°Ð¹Ñ‚ Ð½Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚, Ð¸Ð»Ð¸ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚.')\n",
        "\n",
        "# @bot.message_handler(func=lambda message: message.text=='ÐÐ°Ñ‡Ð°Ñ‚ÑŒ')\n",
        "# def rabota(message):\n",
        "#   bot.send_message(message.from_user.id, 'Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÑÑÑ‹Ð»ÐºÑƒ Ð½Ð° Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ')\n",
        "#   bot.register_next_step_handler(message, test);\n",
        "# def test(message):\n",
        "#     bot.send_message(message.chat.id, 'ÐŸÐ¾Ð´Ð¾Ð¶Ð´Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐµÐºÑƒÐ½Ð´, Ð¸Ð´ÑƒÑ‚ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ...')\n",
        "#     link=message.text\n",
        "#     predict=fakeNewsChecking(link)\n",
        "#     bot.send_message(message.chat.id,text=predict)\n",
        "# def proga(m):\n",
        "#   bot.send_message(m.from_user.id, 'ÑÑÑ‹Ð»ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ÑÑ')\n",
        "# @bot.message_handler(func=lambda message: message.text=='Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ')\n",
        "# def info(message):\n",
        "#   bot.send_message(message.from_user.id, \"Ð­Ñ‚Ð¾ Ð¿Ñ€Ð¾ÐµÐºÑ‚ ÑƒÑ‡Ð°Ñ‰Ð¸Ñ…ÑÑ IT-ÐºÐ»Ð°ÑÑÐ° Ð»Ð¸Ñ†ÐµÑ â„–1511 Ð¿Ñ€Ð¸ ÐœÐ˜Ð¤Ð˜, ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ð´Ð»Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹ Ð¾Ñ‚ Ñ„ÐµÐ¹ÐºÐ¾Ð²Ñ‹Ñ… Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ Ð² ÑÐµÑ‚Ð¸.\")\n",
        "# @bot.message_handler(func=lambda message: message.text=='ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ')\n",
        "# def help(message):\n",
        "#   bot.send_message(message.from_user.id, \"Ð•ÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ ÐµÑÑ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾ Ð±Ð¾Ñ‚Ñƒ Ð¸Ð»Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹, Ñ‚Ð¾ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð¸Ñ… Ð¼Ð¾Ð¶Ð½Ð¾ Ð·Ð´ÐµÑÑŒ @miCherex\")\n",
        "\n",
        "bot.polling() "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jtJS8uVs-LbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "telebot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}